â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€

      ðŸ›°ï¸  REPOSITORY ORBIT: github.com/n6s8/rag-quality-gates
      ðŸ“¡ SIGNAL STRENGTH: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

      ðŸŽ¬ TRANSMISSION FEED: youtu.be/HgSonhJaUoU
      ðŸ“¶ BANDWIDTH: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„

# ðŸ“œ Historical Quotes Explorer â€” Advanced RAG + Evaluation & Enhancement

A complete **Retrieval-Augmented Generation (RAG)** system for exploring historical quotes with AI-powered context and explanations â€” plus an **automated evaluation pipeline** that measures RAG metrics, applies an enhancement, and generates a Markdown report.

You can ask:

- â€œWhat did Roosevelt say about fear?â€
- â€œWhat did Martin Luther King Jr. dream about?â€
- â€œWho said â€˜Be the change you wish to see in the worldâ€™?â€
- â€œShow me quotes about perseverance or leadershipâ€

â€¦and the system returns:

- an **LLM answer** grounded in the dataset
- the **exact retrieved quotes + metadata** used as context
- (for the Advanced task) **metrics + report** proving improvement after enhancement

---

## âœ… Assignment Alignment (Advanced RAG Practical Task)

This repository includes:

1) **Metrics definition for RAG**
- Valuable RAG metrics are implemented and measured automatically.
- We selected **Retrieval Precision** as the primary target metric (high business value: less irrelevant context â†’ more trustworthy answers).

2) **Automated testing environment**
- Scripts measure metrics under evaluation queries and store machine-readable artifacts (`.json`).
- A report generator builds `docs/enhancement_report.md` from those artifacts.

3) **System enhancement**
- An enhancement was applied to improve the target metric.
- The report includes baseline vs enhanced comparison and trade-offs.

4) **Re-evaluation + appended reporting**
- Baseline and enhanced runs are repeated with the same evaluation setup.
- The report is updated with the new state.

âœ… **Acceptance criterion met:** Retrieval Precision improved by **+128.6%** (>= +30%).

---

## ðŸ§  System Architecture (Core RAG)

### 1) Dataset
- Domain: historical quotes (Roosevelt, MLK, Gandhi, Mandela, etc.)
- Data files:
  - `data/quotes_dataset.json` â€” quote text + author + era + topic + tags + context + source
  - (optional) `data/historical_context.json` â€” author metadata / bios

### 2) Vector Database
- Vector DB: **Qdrant**
- Collection: `historical_quotes`
- Vector size: 384
- Distance: cosine

### 3) Embeddings
- SentenceTransformers: `all-MiniLM-L6-v2`
- Used for:
  - embedding quotes during ingestion
  - embedding user queries at runtime

### 4) Ingestion (Load data into Qdrant)
- Script: `src/database/data_loader.py`
  - reads JSON
  - creates embeddings
  - upserts into Qdrant

### 5) LLM Client
- Local HF model supported (example used in runs):
  - `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (CPU)
- Generates answers from retrieved context.

### 6) UI (Optional)
- Streamlit interface: `frontend/app.py`
- Lets users ask questions and view retrieved quotes.

### 7) RAG Pipeline
- File: `rag/rag_pipeline_rest.py`
- Steps:
  - embed query
  - vector search in Qdrant
  - build prompt/context from retrieved docs
  - LLM generation
  - return answer + evidence

---

## ðŸ“ Metrics (Advanced Task)

The evaluation pipeline tracks:

### Core retrieval metrics
- **Retrieval Precision** (TARGET) â€” how many retrieved docs are actually relevant
- Retrieval Recall â€” how many relevant docs were successfully retrieved

### Answer quality metrics (heuristics)
- Answer Relevance
- Hallucination Score
- Response Time

### Optional analysis/interpretation metrics (heuristics)
- Interpretation Score
- Historical Context Score
- Explanation Depth
- Thematic Analysis
- Interpretation Quality

> Note: interpretation metrics are heuristic and mainly useful for regression comparisons.

---

## ðŸš€ Quick Start (Run the App)

### âœ… Prerequisites
- Python 3.8+
- Git
- Docker Desktop / Docker Engine
- Internet (first run downloads models)

### ðŸ“¦ 1) Clone + Install
```bash
git clone <repository-url>
cd rag-historical-quotes
pip install -r requirements.txt
```

## ðŸš€ Quick Start (Run the App)

### ðŸ§± 2) Start Qdrant
```bash
docker-compose -f docker/docker-compose.yml up -d
```
### ðŸ©º 3) Check Qdrant Health
```bash
python -c "import requests; print(requests.get('http://localhost:6333/health').text)"
```
### ðŸ“¥ 4. Load Quotes into Qdrant
```bash
python src/database/data_loader.py
```
### ðŸ”Ž 5. Optional: Quick Retrieval Test
```bash
python test_search.py
```
### â–¶ï¸ 6. Run UI (Streamlit)
```bash
streamlit run frontend/app.py
```
## ðŸ§ª Advanced Task: Automated Evaluation + Report

### âœ… One-command full evaluation (baseline + enhanced + report)

```bash
python scripts/run_full_evaluation.py
```
### ðŸ“Š Current Results (from the latest run)

- **Target metric:** Retrieval Precision  
- **Baseline:** 0.389  
- **Enhanced:** 0.889  
- **Improvement:** **+128.6%** âœ… (>= +30%)
